<?xml version="1.0"?>
<rss version="2.0">
<channel>
<title>Karl Arao's TiddlyWiki</title>
<link>http://karlarao.tiddlyspot.com</link>
<description></description>
<language>en</language>
<copyright>Copyright 2020 YourName</copyright>
<pubDate>Tue, 18 Aug 2020 18:31:18 GMT</pubDate>
<lastBuildDate>Tue, 18 Aug 2020 18:31:18 GMT</lastBuildDate>
<docs>http://blogs.law.harvard.edu/tech/rss</docs>
<generator>TiddlyWiki 2.5.0</generator>
<item>
<title>.Looker</title>
<description></description>
<category>Visualization</category>
<category>GCP essentials hands-on</category>
<link>http://karlarao.tiddlyspot.com#.Looker</link>
<pubDate>Tue, 18 Aug 2020 12:40:00 GMT</pubDate>

</item>
<item>
<title>bq CDC</title>
<description>Sync Oracle to &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; With Golden Gate &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; Adapter &lt;a target=&quot;_blank&quot; title=&quot;External link to https://medium.com/searce/sync-oracle-to-bigquery-with-golden-gate-bigquery-adapter-59991bbdb5e3&quot; href=&quot;https://medium.com/searce/sync-oracle-to-bigquery-with-golden-gate-bigquery-adapter-59991bbdb5e3&quot; class=&quot;externalLink&quot;&gt;https://medium.com/searce/sync-oracle-to-bigquery-with-golden-gate-bigquery-adapter-59991bbdb5e3&lt;/a&gt;&lt;br&gt;</description>
<category>bq migration</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20CDC%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 17:11:00 GMT</pubDate>

</item>
<item>
<title>gcp bq commandline</title>
<description>&lt;div class=&quot;dcTOC&quot;&gt;&lt;a class=&quot;toggleButton&quot; title=&quot;show/collapse table of contents&quot; href=&quot;javascript:;&quot;&gt;/* Table of Contents */&lt;/a&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' READ THIS ', event)&quot;&gt; READ THIS &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' show current Cloud SDK config', event)&quot;&gt; show current Cloud SDK config&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' create dataset ', event)&quot;&gt; create dataset &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' create dataset on different project ', event)&quot;&gt; create dataset on different project &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' create table ', event)&quot;&gt; create table &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load file to table ', event)&quot;&gt; load file to table &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' auto schema detection ', event)&quot;&gt; auto schema detection &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' file limit on local laptop upload 100MB ', event)&quot;&gt; file limit on local laptop upload 100MB &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - fixing null values', event)&quot;&gt; load - fixing null values&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - replace existing table', event)&quot;&gt; load - replace existing table&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - skip first lines of header', event)&quot;&gt; load - skip first lines of header&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' sed to replace all occurrences of PrivacySuppressed by NULL, compressing the result and writing it to a temporary folder', event)&quot;&gt; sed to replace all occurrences of PrivacySuppressed by NULL, compressing the result and writing it to a temporary folder&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - specify schema metadata file ', event)&quot;&gt; load - specify schema metadata file &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - CTAS ', event)&quot;&gt; load - CTAS &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - stage first on GCS then load to bigquery ', event)&quot;&gt; load - stage first on GCS then load to bigquery &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - create EXTERNAL TEMPORARY TABLE, generate METADATA file from CSV on GCS', event)&quot;&gt; load - create EXTERNAL TEMPORARY TABLE, generate METADATA file from CSV on GCS&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - handling NULL', event)&quot;&gt; load - handling NULL&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' load - jagged rows ', event)&quot;&gt; load - jagged rows &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' loading files BEST PRACTICE', event)&quot;&gt; loading files BEST PRACTICE&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' running SQL file using FLAGFILE - get_metadata.sql', event)&quot;&gt; running SQL file using FLAGFILE - get_metadata.sql&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' running SQL file, remove headers, GENERATE METADATA FROM BQ to JSON', event)&quot;&gt; running SQL file, remove headers, GENERATE METADATA FROM BQ to JSON&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' running SQL file, remove headers, GENERATE METADATA FROM BQ to JSON - BETTER SIMPLER WAY', event)&quot;&gt; running SQL file, remove headers, GENERATE METADATA FROM BQ to JSON - BETTER SIMPLER WAY&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' generate-schema - AUTO GENERATE METADATA', event)&quot;&gt; generate-schema - AUTO GENERATE METADATA&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' running SQL file, specify number of rows output ', event)&quot;&gt; running SQL file, specify number of rows output &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' running SQL file - run file from GCS bucket', event)&quot;&gt; running SQL file - run file from GCS bucket&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' running SQL - using bash ', event)&quot;&gt; running SQL - using bash &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' data dictionary - get schema of all the tables in the dataset ', event)&quot;&gt; data dictionary - get schema of all the tables in the dataset &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' data dictionary - bq - list tables of dataset ', event)&quot;&gt; data dictionary - bq - list tables of dataset &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' data dictionary - bq - show column data type, rows, and table size ', event)&quot;&gt; data dictionary - bq - show column data type, rows, and table size &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' data dictionary - bq - show ACL of dataset ', event)&quot;&gt; data dictionary - bq - show ACL of dataset &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' DDL - efficient CLONE of a table - bq cp ', event)&quot;&gt; DDL - efficient CLONE of a table - bq cp &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' DDL - delete table , delete dataset ', event)&quot;&gt; DDL - delete table , delete dataset &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' DDL - specify that a table needs to be expired at a certain time in the future', event)&quot;&gt; DDL - specify that a table needs to be expired at a certain time in the future&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' DML - delete , insert ', event)&quot;&gt; DML - delete , insert &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SELECT - Switching SQL dialects', event)&quot;&gt; SELECT - Switching SQL dialects&lt;/a&gt;&lt;/span&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' using bq parameter ', event)&quot;&gt; using bq parameter &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' Using a query prefix in the UI', event)&quot;&gt; Using a query prefix in the UI&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' set default in bigqueryrc ', event)&quot;&gt; set default in bigqueryrc &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SQL TUNING - get jobs running on a project ', event)&quot;&gt; SQL TUNING - get jobs running on a project &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SQL TUNING - show job info ', event)&quot;&gt; SQL TUNING - show job info &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SQL TUNING - show job details ', event)&quot;&gt; SQL TUNING - show job details &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SQL TUNING - generate json execution plan ', event)&quot;&gt; SQL TUNING - generate json execution plan &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SQL TUNING - compare two job_ids', event)&quot;&gt; SQL TUNING - compare two job_ids&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' backup and restore dataset and tables ', event)&quot;&gt; backup and restore dataset and tables &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' bq insert to table ', event)&quot;&gt; bq insert to table &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' bq extract from table ', event)&quot;&gt; bq extract from table &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SCRIPTING - data types', event)&quot;&gt; SCRIPTING - data types&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SCRIPTING - use bq CLI to execute a query with bind parameters', event)&quot;&gt; SCRIPTING - use bq CLI to execute a query with bind parameters&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SCRIPTING - parameterized queries using the Python Cloud Client API', event)&quot;&gt; SCRIPTING - parameterized queries using the Python Cloud Client API&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; READ THIS &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/bq-command-line-tool&quot; href=&quot;https://cloud.google.com/bigquery/docs/bq-command-line-tool&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/bq-command-line-tool&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; show current Cloud SDK config&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;gcloud config list
bq show
bq ls -p
bq ls
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; create dataset &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq --location=US mk ch04

bq show ch04 

bq mk --location=US \
      --default_table_expiration 3600 \
      --description &quot;Chapter 5 of BigQuery Book.&quot; \
      ch05
&lt;/pre&gt;&lt;br&gt;&lt;h1&gt; create dataset on different project &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq mk --location=US \
       --default_table_expiration 3600 \
       --description &quot;Chapter 5 of BigQuery Book.&quot; \
       projectname:ch05
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; create table &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;Table Path
    [[project_name.]dataset_name.]table_name

    project_name is the name of the project where you are creating the table.
      Defaults to the project that runs this DDL query.
      If the project name contains special characters such as colons, it should be quoted in backticks ` (example: `google.com:my_project`).

    dataset_name is the name of the dataset where you are creating the table.
      Defaults to the defaultDataset in the request.

    table_name is the name of the table you're creating.
      Must be unique per dataset.
      Can contain up to 1,024 characters (upper or lower case letters), numbers, and underscores
&lt;/pre&gt;&lt;pre&gt;bq mk --table \
    --expiration 3600 \
    --description &quot;One hour of data&quot; \
     --label persistence:volatile \
     ch05.rentals_last_hour rental_id:STRING,duration:FLOAT


# using a metadata file schema.json  
bq mk --table \
    --expiration 3600 \
    --description &quot;One hour of data&quot; \
     --label persistence:volatile \
     ch05.rentals_last_hour schema.json


# this shows the pretty output 
bq show ch04.college_scorecard

# export metadata
bq show --schema --format=prettyjson ch04.college_scorecard &amp;gt; schema.json

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load file to table &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/loading-data-local#loading_data_from_a_local_data_source&quot; href=&quot;https://cloud.google.com/bigquery/docs/loading-data-local#loading_data_from_a_local_data_source&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/loading-data-local#loading_data_from_a_local_data_source&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_new_table&quot; href=&quot;https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_new_table&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_new_table&lt;/a&gt;&lt;br&gt;&lt;pre&gt;                Examples:
                bq load ds.new_tbl ./info.csv ./info_schema.json
                bq load ds.new_tbl gs://mybucket/info.csv ./info_schema.json
                bq load ds.small gs://mybucket/small.csv name:integer,value:string
                bq load ds.small gs://mybucket/small.csv field1,field2,field3

                Arguments:
                destination_table: Destination table name.
                source: Name of local file to import, or a comma-separated list of
                URI paths to data to import.
                schema: Either a text schema or JSON file, as above.

&lt;/pre&gt;&lt;pre&gt;
# compressed from local laptop (66seconds)
bq --location=US \
   load \
   --source_format=CSV --autodetect \
   ch04.college_scorecard \
   ./college_scorecard.csv.gz


# compressed from cloudshell (65seconds)
bq --location=US \
   load \
   --source_format=CSV --autodetect \
   ch05.college_scorecard \
   ./college_scorecard.csv.gz


# uncompressed from local laptop 
# (934 rows, 18MB, 15seconds)
# (1552 rows, 30MB, 23seconds)
# (2586 rows, 50MB, 34seconds)
# (5134 rows, 100MB, 66seconds)
bq --location=US \
   load \
   --source_format=CSV --autodetect \
   ch06.college_scorecard \
   ./fileaa



# when the file is too big it errors with, workaround is to compress or put it on gcs
BigQuery error in load operation: Could not connect with BigQuery server due to: RedirectMissingLocation('Redirected but the response is missing a Location:
header.')

# the limit of CSV file from laptop is 100MB 
split -b 103424k college_scorecard.csv abc   &amp;lt;- this 101MB
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; auto schema detection &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/schema-detect&quot; href=&quot;https://cloud.google.com/bigquery/docs/schema-detect&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/schema-detect&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; file limit on local laptop upload 100MB &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ split -b 103424k college_scorecard.csv abc
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ ls -ltr
total 557168
-rw-rw-r--  1 kristofferson.a.arao  562225435  142634461 Jul  6 02:38 college_scorecard.csv
-rw-r--r--  1 kristofferson.a.arao  562225435  105906176 Jul 26 19:22 abcaa
-rw-r--r--  1 kristofferson.a.arao  562225435   36728285 Jul 26 19:22 abcab
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ 
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ 
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ bq --location=US    load    --source_format=CSV --autodetect    ch06.college_scorecard    ./abcaa
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
BigQuery error in load operation: Could not connect with BigQuery server due to: RedirectMissingLocation('Redirected but the response is missing a Location:
header.')
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ split -b 102400k college_scorecard.csv xyz
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ 
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ ls -ltr
total 835752
-rw-rw-r--  1 kristofferson.a.arao  562225435  142634461 Jul  6 02:38 college_scorecard.csv
-rw-r--r--  1 kristofferson.a.arao  562225435  105906176 Jul 26 19:22 abcaa
-rw-r--r--  1 kristofferson.a.arao  562225435   36728285 Jul 26 19:22 abcab
-rw-r--r--  1 kristofferson.a.arao  562225435  104857600 Jul 26 19:24 xyzaa
-rw-r--r--  1 kristofferson.a.arao  562225435   37776861 Jul 26 19:24 xyzab
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ 
(py385) AMAC02T60SJH03Y:college_scorecard kristofferson.a.arao$ bq --location=US    load    --source_format=CSV --autodetect    ch06.college_scorecard    ./xyzaa
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Upload complete.
Waiting on bqjob_r54bdb0a90f03ea75_000001738d712ab4_1 ... (49s) Current status: DONE   

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load - fixing null values&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;blockquote&gt;Could not parse 'NULL' as int for field HBCU (position 26) starting at location&lt;br&gt;11945910&lt;br&gt;&lt;br&gt;This caused the load job to fail with the following error&lt;br&gt;&lt;br&gt;CSV table encountered too many errors, giving up. Rows: 591; errors: 1.&lt;br&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;pre&gt;* we could edit the data file itself if we knew what the value ought to be.
* specify explicitly the schema for each column and change the column type of the HBCU column to be a string so that NULL is an acceptable value. 
* we could ask BigQuery to ignore a few bad records by specifying, for example, --max_bad_records=20
* we could instruct the BigQuery load program that this particular file uses the string NULL to mark nulls (the standard way in CSV is to use empty fields to represent nulls)
&lt;/pre&gt;&lt;br&gt;&lt;pre&gt;bq --location=US \
   load --null_marker=NULL \
   --source_format=CSV --autodetect \
   ch04.college_scorecard \
   ./college_scorecard.csv.gz
&lt;/pre&gt;&lt;br&gt;&lt;h1&gt; load - replace existing table&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;you want to replace the existing table, so you should add --replace:

bq --location=US \
   load --null_marker=NULL --replace \
   --source_format=CSV --autodetect \
   ch04.college_scorecard \
   ./college_scorecard.csv.gz

You can also specify --replace=false to append rows to an existing table
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load - skip first lines of header&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;ul&gt;&lt;li&gt; skip_leading_rows is for load, not query&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;bq load --source_format=CSV --skip_leading_rows=1

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; sed to replace all occurrences of &lt;a tiddlylink=&quot;PrivacySuppressed&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#PrivacySuppressed&quot; href=&quot;http://karlarao.tiddlyspot.com#PrivacySuppressed&quot; class=&quot;externalLink null&quot;&gt;PrivacySuppressed&lt;/a&gt; by NULL, compressing the result and writing it to a temporary folder&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;zless ./college_scorecard.csv.gz | \
        sed 's/PrivacySuppressed/NULL/g' | \
        gzip &amp;gt; /tmp/college_scorecard.csv.gz
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load - specify schema metadata file &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;ul&gt;&lt;li&gt; edit the schema.json and make necessary data type changes&lt;/li&gt;&lt;li&gt; Because we are supplying a schema, we need to instruct &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; to ignore the first row of the CSV file (which contains the header information).&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;bq --location=US \
   load --null_marker=NULL --replace \
   --source_format=CSV \
   --schema=schema.json --skip_leading_rows=1 \
   ch06.college_scorecard \
   ./college_scorecard.csv.gz
&lt;/pre&gt;&lt;br&gt;&lt;pre&gt;BEFORE QUERY - suppressing errors 

SELECT
  INSTNM
  , ADM_RATE_ALL
  , FIRST_GEN
  , MD_FAMINC
  , MD_EARN_WNE_P10
  , SAT_AVG
FROM
  ch04.college_scorecard
WHERE
  SAFE_CAST(SAT_AVG AS FLOAT64) &amp;gt; 1300
  AND SAFE_CAST(ADM_RATE_ALL AS FLOAT64) &amp;lt; 0.2
  AND SAFE_CAST(FIRST_GEN AS FLOAT64) &amp;gt; 0.1
ORDER BY
  CAST(MD_FAMINC AS FLOAT64) ASC

AFTER QUERY 

SELECT
  INSTNM
  , ADM_RATE_ALL
  , FIRST_GEN
  , MD_FAMINC
  , MD_EARN_WNE_P10
  , SAT_AVG
FROM
  ch04.college_scorecard
WHERE
  SAT_AVG &amp;gt; 1300
  AND ADM_RATE_ALL &amp;lt; 0.2
  AND FIRST_GEN &amp;gt; 0.1
ORDER BY
  MD_FAMINC ASC

Notice that, because SAT_AVG, ADM_RATE_ALL, and the others are no longer strings, our query is much cleaner because we no longer need to cast them to floating-point numbers. The reason they are no longer strings is that we made a decision on how to deal with the privacy-suppressed data (treat them as being unavailable) during the Extract, Transform, and Load (ETL) process.
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load - CTAS &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;ul&gt;&lt;li&gt; data type gets copied &lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;CREATE OR REPLACE TABLE ch04.college_scorecard_etl AS
 SELECT 
    INSTNM
    , ADM_RATE_ALL
    , FIRST_GEN
    , MD_FAMINC
    , SAT_AVG
    , MD_EARN_WNE_P10
 FROM ch04.college_scorecard
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load - stage first on GCS then load to bigquery &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;br&gt;&lt;pre&gt;--# multi-thread copy files from local 
gsutil -m cp *.csv gs://BUCKET/some/location


--# load from bucket to bigquery
bq load … gs://BUCKET/some/location/*.csv
&lt;/pre&gt;&lt;br&gt;&lt;pre&gt;bq load --source_format=NEWLINE_DELIMITED_JSON example-dev-284123:cpb200_flight_data.flights_2014 gs://cloud-training/CPB200/BQ/lab4/domestic_2014_flights_*.json ./schema_flight_performance.json
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Waiting on bqjob_r263b24173998e9e9_000001739049ffa5_1 ... (49s) Current status: DONE  
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load - create EXTERNAL TEMPORARY TABLE, generate METADATA file from CSV on GCS&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://codelabs.developers.google.com/codelabs/cpb200-loading-data/#8&quot; href=&quot;https://codelabs.developers.google.com/codelabs/cpb200-loading-data/#8&quot; class=&quot;externalLink&quot;&gt;https://codelabs.developers.google.com/codelabs/cpb200-loading-data/#8&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stevenlevine.dev/2019/11/querying-externally-partitioned-data-with-bq/&quot; href=&quot;https://stevenlevine.dev/2019/11/querying-externally-partitioned-data-with-bq/&quot; class=&quot;externalLink&quot;&gt;https://stevenlevine.dev/2019/11/querying-externally-partitioned-data-with-bq/&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;pre&gt;gsutil ls gs://example_bucket-dev/flights
gs://example_bucket-dev/flights/
gs://example_bucket-dev/flights/flights_airports_000000000000


--# generate metadata file from CSV
bq mkdef --source_format=CSV --autodetect &quot;gs://example_bucket-dev/flights/flights_airports_*&quot;  &amp;gt; mytable.json


cat mytable.json 
{
  &quot;autodetect&quot;: true,
  &quot;csvOptions&quot;: {
    &quot;encoding&quot;: &quot;UTF-8&quot;,
    &quot;quote&quot;: &quot;\&quot;&quot;
  },
  &quot;sourceFormat&quot;: &quot;CSV&quot;,
  &quot;sourceUris&quot;: [
    &quot;gs://example_bucket-dev/flights/flights_airports_*&quot;
  ]



--# create external table using metadata file 
bq mk --external_table_definition=mytable.json cpb200_flight_data.flights_airports
Table 'example-dev-284123:cpb200_flight_data.flights_airports' successfully created.

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load - handling NULL&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq load --null_marker=&quot;NULL&quot; 
&lt;/pre&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/46001334/bigquery-load-null-is-treating-as-string-instead-of-empty&quot; href=&quot;https://stackoverflow.com/questions/46001334/bigquery-load-null-is-treating-as-string-instead-of-empty&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/46001334/bigquery-load-null-is-treating-as-string-instead-of-empty&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; load - jagged rows &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/27024330/how-to-detect-jagged-rows-in-bigquery&quot; href=&quot;https://stackoverflow.com/questions/27024330/how-to-detect-jagged-rows-in-bigquery&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/27024330/how-to-detect-jagged-rows-in-bigquery&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/43495592/bigquery-csv-import-allow-jagged-rows?rq=1&quot; href=&quot;https://stackoverflow.com/questions/43495592/bigquery-csv-import-allow-jagged-rows?rq=1&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/43495592/bigquery-csv-import-allow-jagged-rows?rq=1&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; loading files BEST PRACTICE&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;br&gt;On schema metadata &lt;br&gt;&lt;ul&gt;&lt;li&gt; It is therefore best practice to not autodetect the schema of files that you receive in production—you will be at the mercy of whatever data happens to have been sampled. For production workloads, insist on the data type for a column by specifying it at the time of load.&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;br&gt;On file format&lt;br&gt;&lt;ul&gt;&lt;li&gt; CSV files are inefficient and not very expressive (for example, there is no way to represent arrays and structs in CSV&lt;/li&gt;&lt;li&gt; An efficient and expressive format is Avro. Avro uses self-describing binary files that are broken into blocks and can be compressed block by block. Because of this, it is possible to parallelize the loading of data from Avro files and the export of data into Avro files. Because the blocks are compressed, the file sizes will also be smaller than the data size might indicate. In terms of expressiveness, the Avro format is hierarchical and can represent nested and repeated fields, something that &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; supports but CSV files don’t have an easy way to store. Because Avro files are self-describing, you never need to specify a schema.&lt;ul&gt;&lt;li&gt; There are two drawbacks to Avro files. One is that they are not human readable. If readability and expressiveness are important to you, use newline-delimited JSON files to store your data&lt;/li&gt;&lt;li&gt; The second drawback is that Avro files are stored row by row. This makes Avro files not as efficient for federated queries.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt; JSON supports the ability to store hierarchical data but requires that binary columns be base-64 encoded. However, JSON files are larger than even the equivalent CSV files because the name of each field is repeated on every line.&lt;/li&gt;&lt;li&gt; The Parquet file format was inspired by Google’s original Dremel &lt;a tiddlylink=&quot;ColumnIO&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#ColumnIO&quot; href=&quot;http://karlarao.tiddlyspot.com#ColumnIO&quot; class=&quot;externalLink null&quot;&gt;ColumnIO&lt;/a&gt; format and like Avro, Parquet is binary, block oriented, compact, and capable of representing hierarchical data. However, whereas Avro files are stored row by row, Parquet files are stored column by column. Columnar files are optimized for reading a subset of the columns; loading data requires reading all columns, and so columnar formats are somewhat less efficient at the loading of data. However, the columnar format makes Parquet a better choice than Avro for federated queries, a topic that we discuss shortly. Optimized Row Columnar (ORC) files are another open source columnar file format. ORC is similar to Parquet in performance and efficiency.&lt;/li&gt;&lt;li&gt; in summary - Therefore, if you have a choice of file formats, we recommend Avro if you plan to load the data into &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; and discard the files. We recommend Parquet if you will be retaining the files for federated queries. Use JSON for small files where human readability is important.&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;br&gt;On compression&lt;br&gt;&lt;ul&gt;&lt;li&gt; CSV and JSON that do not have internal compression, you should consider whether you should compress the files using gzip. Compressed files are faster to transmit and take up less space, but they are slower to load into &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt;. The slower your network, the more you should lean toward compressing the data.&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;br&gt;On staging to GCS before loading to bigquery&lt;br&gt;&lt;ul&gt;&lt;li&gt; Staging the file on Google Cloud Storage involves paying storage costs at least until the &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; load job finishes. However, storage costs are generally quite low and so, on this dataset and this network connection, the best option is to stage compressed data in Cloud Storage and load it from there. Even though it is faster to load uncompressed files into &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt;, the network time to transfer the files dwarfs whatever benefits you’d get from a faster load.&lt;/li&gt;&lt;li&gt; As of this writing, the loading of compressed CSV and JSON files is limited to files less than 4 GB in size because &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; has to uncompress the files on the fly on workers whose memory is finite. If you have larger datasets, split them across multiple CSV or JSON files. Splitting files yourself can allow for some degree of parallelism when doing the loads, but depending on how you size the files, this can lead to suboptimal file sizes in the table until &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; decides to optimize the storage.&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;On loading LIMITATIONS and PRICING &lt;br&gt;&lt;ul&gt;&lt;li&gt; &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; does not charge for loading data. Ingestion happens on a set of workers that is distinct from the cluster providing the slots used for querying. Hence, your queries (even on the same table into which you are ingesting data) are not slowed down by the fact that data is being ingested.&lt;/li&gt;&lt;li&gt; Data loads are atomic. Queries on a table will either reflect the presence of all the data that is loaded in through the bq load operation or reflect none of it. You will not get query results on a partial slice of the data.&lt;/li&gt;&lt;li&gt; The drawback of loading data using a “free” cluster is that load times can become unpredictable and bottlenecked by preexisting jobs. As of this writing, load jobs are limited to 1,000 per table and 100,000 per project per day. In the case of CSV and JSON files, cells and rows are limited to 100 MB, whereas in Avro, blocks are limited to 16 MB. Files cannot exceed 5 TB in size. If you have a larger dataset, split it across multiple files, each smaller than 5 TB. However, a single load job can submit a maximum of 15 TB of data split across a maximum of 10 million files. The load job must finish executing in less than six hours or it will be cancelled.&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; running SQL file using FLAGFILE - get_metadata.sql&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;  --format: &amp;lt;none|json|prettyjson|csv|sparse|pretty&amp;gt;: Format for command output.
    Options include:
    pretty: formatted table output
    sparse: simpler table output
    prettyjson: easy-to-read JSON format
    json: maximally compact JSON
    csv: csv format with header
    The first three are intended to be human-readable, and the latter three are
    for passing to another program. If no format is selected, one will be chosen
    based on the command run.

&lt;/pre&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt; get_metadata&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;cat get_metadata.sql 

SELECT 
 TO_JSON_STRING(
    ARRAY_AGG(STRUCT( 
      IF(is_nullable = 'YES', 'NULLABLE', 'REQUIRED') AS
mode,
      column_name AS name,
      data_type AS type)
    ORDER BY ordinal_position), TRUE) AS schema
FROM
  ch04.INFORMATION_SCHEMA.COLUMNS
WHERE
  table_name = 'college_scorecard';  




bq query --format=sparse --use_legacy_sql=false --flagfile=get_metadata.sql
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; running SQL file, remove headers, GENERATE METADATA FROM BQ to JSON&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/schemas#specify-schema-manual-python&quot; href=&quot;https://cloud.google.com/bigquery/docs/schemas#specify-schema-manual-python&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/schemas#specify-schema-manual-python&lt;/a&gt;&lt;br&gt;&lt;pre&gt;bq query --format=sparse --use_legacy_sql=false --flagfile=get_metadata.sql | awk 'NR&amp;gt;2' &amp;gt; schema2.json

the difference 
                      schema                                  &amp;lt;
 -------------------------------------------------            &amp;lt;
  [                                                               [                                                
    {                                                               {                                              
      &quot;mode&quot;: &quot;NULLABLE&quot;,                                             &quot;mode&quot;: &quot;NULLABLE&quot;,                          
      &quot;name&quot;: &quot;UNITID&quot;,                                               &quot;name&quot;: &quot;UNITID&quot;,                            
      &quot;type&quot;: &quot;INT64&quot;                                                 &quot;type&quot;: &quot;INT64&quot;                              
    },                                                              },                                             
    {                                                               {                                              
      &quot;mode&quot;: &quot;NULLABLE&quot;,                                             &quot;mode&quot;: &quot;NULLABLE&quot;,                          
      &quot;name&quot;: &quot;OPEID&quot;,                                                &quot;name&quot;: &quot;OPEID&quot;,                             
      &quot;type&quot;: &quot;INT64&quot;                                                 &quot;type&quot;: &quot;INT64&quot;                              
    },                                                              },                               
&lt;/pre&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/45415564/remove-header-from-query-result-in-bq-command-line&quot; href=&quot;https://stackoverflow.com/questions/45415564/remove-header-from-query-result-in-bq-command-line&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/45415564/remove-header-from-query-result-in-bq-command-line&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/33426395/google-bigquery-bq-command-line-execute-query-from-a-file&quot; href=&quot;https://stackoverflow.com/questions/33426395/google-bigquery-bq-command-line-execute-query-from-a-file&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/33426395/google-bigquery-bq-command-line-execute-query-from-a-file&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; running SQL file, remove headers, GENERATE METADATA FROM BQ to JSON - BETTER SIMPLER WAY&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;# this shows the pretty output 
bq show ch04.college_scorecard

# export metadata
bq show --schema --format=prettyjson ch04.college_scorecard &amp;gt; schema.json
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; generate-schema - AUTO GENERATE METADATA&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;blockquote&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://github.com/bxparks/bigquery-schema-generator&quot; href=&quot;https://github.com/bxparks/bigquery-schema-generator&quot; class=&quot;externalLink&quot;&gt;https://github.com/bxparks/bigquery-schema-generator&lt;/a&gt;&lt;br&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt; need to remove header because INT will be evaluated as STRING&lt;/li&gt;&lt;li&gt; only supports CSV, not pipe delimited data. You can sample and replace pipe w/ csv as a workaround &lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;pip install bigquery_schema_generator


--# remove 1st line header
sed 1d LOAN_STATUS_SED_OFFER-pipe.csv | generate-schema --input_format csv


--# sample line 10 to 42 of the file 
sed -n '10,42p' LOAN_STATUS_SED_OFFER-pipe.csv | generate-schema --input_format csv


--#  sample and replace pipe w/ csv as a workaround
sed -n '10,42p' LOAN_STATUS_SED_OFFER-pipefile.csv | sed 's/|/,/g' | generate-schema --input_format csv
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; running SQL file, specify number of rows output &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;$ bq query -n=0 --use_legacy_sql=false &quot;SELECT x FROM UNNEST([1, 2, 3]) AS x;&quot;
Waiting on &amp;lt;job id&amp;gt; ... (0s) Current status: DONE
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; running SQL file - run file from GCS bucket&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;gsutil cp get_metadata.sql gs://example_bucket-dev/scripts/

bq query --format=sparse --use_legacy_sql=false &quot;$(gsutil cat gs://example_bucket-dev/scripts/get_metadata.sql)&quot;
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; running SQL - using bash &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;cat bq_query.sh 
#!/bin/bash

read -d '' QUERY_TEXT &amp;lt;&amp;lt; EOF
SELECT 
  start_station_name
  , AVG(duration) as duration
  , COUNT(duration) as num_trips
FROM \`bigquery-public-data\`.london_bicycles.cycle_hire
GROUP BY start_station_name 
ORDER BY num_trips DESC 
LIMIT 5
EOF

bq query --use_legacy_sql=false $QUERY_TEXT

&lt;/pre&gt;&lt;br&gt;&lt;pre&gt;sh bq_query.sh 

+-------------------------------------+--------------------+-----------+
|         start_station_name          |      duration      | num_trips |
+-------------------------------------+--------------------+-----------+
| Belgrove Street , King's Cross      | 1011.0766960393734 |    234458 |
| Hyde Park Corner, Hyde Park         |  2782.730708763667 |    215629 |
| Waterloo Station 3, Waterloo        |  866.3761345037934 |    201630 |
| Black Lion Gate, Kensington Gardens | 3588.0120035565997 |    161952 |
| Albert Gate, Hyde Park              | 2359.4139302395806 |    155647 |
+-------------------------------------+--------------------+-----------+
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; data dictionary - get schema of all the tables in the dataset &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;SELECT
  table_name
  , column_name
  , ordinal_position
  , is_nullable
  , data_type
FROM
  ch04.INFORMATION_SCHEMA.COLUMNS
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; data dictionary - bq - list tables of dataset &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq ls cpb200_flight_data
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
    tableId      Type    Labels   Time Partitioning   Clustered Fields  
 -------------- ------- -------- ------------------- ------------------ 
  AIRPORTS       TABLE                                                  
  flights_2014   TABLE                                   

bq ls ch04
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
         tableId          Type    Labels   Time Partitioning   Clustered Fields  
 ----------------------- ------- -------- ------------------- ------------------ 
  college_scorecard       TABLE                                                  
  college_scorecard3      TABLE                                                  
  college_scorecard3a     TABLE                                                  
  college_scorecard_etl   TABLE                          

bq ls ch05
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
       tableId        Type    Labels   Time Partitioning   Clustered Fields  
 ------------------- ------- -------- ------------------- ------------------ 
  college_scorecard   TABLE                                      

&lt;/pre&gt;&lt;br&gt;&lt;h1&gt; data dictionary - bq - show column data type, rows, and table size &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq show cpb200_flight_data.flights_2014
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Table example-dev-284123:cpb200_flight_data.flights_2014

   Last modified                  Schema                 Total Rows   Total Bytes   Expiration   Time Partitioning   Clustered Fields   Labels  
 ----------------- ------------------------------------ ------------ ------------- ------------ ------------------- ------------------ -------- 
  27 Jul 08:41:26   |- YEAR: integer (required)          6303310      1214695872                                                                
                    |- QUARTER: integer (required)                                                                                              
                    |- MONTH: integer (required)                                                                                                
                    |- DAY_OF_MONTH: integer                                                                                                    
                    |- DAY_OF_WEEK: integer                                                                                                     
                    |- FULL_DATE: string                                                                                                        
                    |- CARRIER: string                                                                                                          
                    |- TAIL_NUMBER: string                                                                                                      
                    |- FLIGHT_NUMBER: string                                                                                                    
                    |- ORIGIN: string                                                                                                           
                    |- DESTINATION: string                                                                                                      
                    |- SCHEDULED_DEPART_TIME: integer                                                                                           
                    |- ACTUAL_DEPART_TIME: integer                                                                                              
                    |- DEPARTURE_DELAY: integer                                                                                                 
                    |- TAKE_OFF_TIME: integer                                                                                                   
                    |- LANDING_TIME: integer                                                                                                    
                    |- SCHEDULED_ARRIVAL_TIME: integer                                                                                          
                    |- ACTUAL_ARRIVAL_TIME: integer                                                                                             
                    |- ARRIVAL_DELAY: integer                                                                                                   
                    |- FLIGHT_CANCELLED: integer                                                                                                
                    |- CANCELLATION_CODE: string                                                                                                
                    |- SCHEDULED_ELAPSED_TIME: integer                                                                                          
                    |- ACTUAL_ELAPSED_TIME: integer                                                                                             
                    |- AIR_TIME: integer                                                                                                        
                    |- DISTANCE: integer                                                                                                        
                    |- CARRIER_DELAY: integer                                                                                                   
                    |- WEATHER_DELAY: integer                                                                                                   
                    |- NAS_DELAY: integer                                                                                                       
                    |- SECURITY_DELAY: integer                                                                                                  
                    |- LATE_AIRCRAFT_DELAY: integer                                                                                             

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; data dictionary - bq - show ACL of dataset &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;br&gt;&lt;pre&gt;
bq show cpb200_flight_data

Dataset example-dev-284123:cpb200_flight_data

   Last modified                  ACLs                  Labels  
  27 Jul 08:20:55   Owners:                                     
                      kristofferson.a.arao@gmail.com,           
                      projectOwners                             
                    Writers:                                    
                      projectWriters                            
                    Readers:                                    
                      projectReaders                   

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; DDL - efficient CLONE of a table - bq cp &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt; &quot;bq cp&quot; preserves the REQUIRED (not null) attribute on the column while CTAS does not &lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;pre&gt;
You are not billed for running a query, but you will be billed for the storage of the new table. The bq cp command supports appending (specify -a or --append_table) and replacement (specify -noappend_table).

You can also use the idiomatic Standard SQL method of using either CREATE TABLE AS SELECT or INSERT VALUES, depending on whether the destination already exists. 
However, bq cp is faster (because it copies only the table metadata) and doesn’t incur query costs.



bq cp ch04.college_scorecard ch04.college_scorecard3
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Waiting on bqjob_r3637909fa7bf3265_000001738fa05a55_1 ... (0s) Current status: DONE   
Table 'example-dev-284123:ch04.college_scorecard' successfully copied to 'example-dev-284123:ch04.college_scorecard3'

&lt;/pre&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt; this doubles the number of rows &lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;bq cp -a ch04.college_scorecard ch04.college_scorecard3
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Waiting on bqjob_r6867cc422e6d97fe_000001738fa340cd_1 ... (0s) Current status: DONE   
Table 'example-dev-284123:ch04.college_scorecard' successfully copied to 'example-dev-284123:ch04.college_scorecard3'
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; DDL - delete table , delete dataset &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;
--# delete table 
bq rm ch06.college_scorecard
DROP TABLE IF EXISTS ch04.college_scorecard_gcs;

--# delete dataset  (database)
bq rm -r -f ch06
&lt;/pre&gt;&lt;br&gt;&lt;pre&gt;bq rm ch06.college_scorecard
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
rm: remove table 'example-dev-284123:ch06.college_scorecard'? (y/N) y

bq rm -r -f ch06
/Users/kristofferson.a.arao/gcp-sdk/google-cloud-sdk/platform/bq/bq.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; DDL - specify that a table needs to be expired at a certain time in the future&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;ul&gt;&lt;li&gt; by default  Table expiration &quot;Never&quot;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;It is also possible to specify that a table needs to be expired at a certain time in the future. You can so this with the ALTER TABLE SET OPTIONS statement:

ALTER TABLE ch05.college_scorecard
 SET OPTIONS (
   expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), 
                                       INTERVAL 7 DAY),
   description=&quot;College Scorecard expires seven days from now&quot;
 )
&lt;/pre&gt;&lt;br&gt;&lt;h1&gt; DML - delete , insert &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;DELETE FROM ch04.college_scorecard
WHERE SAT_AVG IS NULL;


INSERT ch04.college_scorecard
SELECT * 
FROM ch04.college_scorecard_etl
WHERE SAT_AVG IS NULL;

INSERT ch04.college_scorecard 
  (INSTNM
     , ADM_RATE_ALL
     , FIRST_GEN
     , MD_FAMINC
     , SAT_AVG
     , MD_EARN_WNE_P10
  )
  VALUES ('abc', 0.1, 0.3, 12345, 1234, 23456),
         ('def', 0.2, 0.2, 23451, 1232, 32456);
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; SELECT - Switching SQL dialects&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/reference/standard-sql/enabling-standard-sql#sql-prefix&quot; href=&quot;https://cloud.google.com/bigquery/docs/reference/standard-sql/enabling-standard-sql#sql-prefix&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/reference/standard-sql/enabling-standard-sql#sql-prefix&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt; using bq parameter &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h2&gt;&lt;pre&gt;bq query \
--use_legacy_sql=false \
'SELECT
  word
FROM
  `bigquery-public-data.samples.shakespeare`'
&lt;/pre&gt;&lt;br&gt;&lt;h2&gt; Using a query prefix in the UI&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h2&gt;&lt;pre&gt;#legacySQL
SELECT
  weight_pounds, state, year, gestation_weeks
FROM
  [bigquery-public-data:samples.natality]
ORDER BY weight_pounds DESC
LIMIT 10;

#legacySQL	Runs the query using legacy SQL
#standardSQL	Runs the query using standard SQL

&lt;/pre&gt;&lt;br&gt;&lt;h2&gt; set default in bigqueryrc &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h2&gt;&lt;br&gt;&lt;pre&gt;set --use_legacy_sql=false in .bigqueryrc 

[query]
--use_legacy_sql=false

[mk]
--use_legacy_sql=false
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; SQL TUNING - get jobs running on a project &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;# get all projects 
bq ls -p
        projectId            friendlyName    
 ------------------------ ------------------ 
  example-dev-284123       example-dev       
  example-prod-284123      example-prod      


# show jobs on a specific project 
bq ls -j example-dev-284123
bq ls -j example-prod-284123

# MUST USE THIS - list job for all users 
bq ls -j -a
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; SQL TUNING - show job info &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq show -j bquxjob_734bad5c_173d4190298

Job example-dev-284123:bquxjob_734bad5c_173d4190298

  Job Type    State      Start Time         Duration                User Email             Bytes Processed   Bytes Billed   Billing Tier   Labels  
 ---------- --------- ----------------- ---------------- -------------------------------- ----------------- -------------- -------------- -------- 
  query      SUCCESS   09 Aug 12:41:22   0:00:03.806000   kristofferson.a.arao@gmail.com   156703034         157286400      1                      
&lt;/pre&gt;&lt;br&gt;&lt;h1&gt; SQL TUNING - show job details &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq --format=prettyjson show -j bquxjob_734bad5c_173d4190298 
&lt;/pre&gt;&lt;br&gt;&lt;h1&gt; SQL TUNING - generate json execution plan &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq --format=prettyjson show -j example-dev-284123:US.bquxjob_27d580de_17387a47aff &amp;gt; myjob.export.json
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; SQL TUNING - compare two job_ids&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq ls -j -a
  import imp
                    jobId                      Job Type    State      Start Time         Duration     
 -------------------------------------------- ---------- --------- ----------------- ---------------- 
  bqjob_r7cd933076d264387_00000173d45a2ad4_1   query      SUCCESS   09 Aug 13:52:27   0:00:00.112000  
  bqjob_r44158463afce6211_00000173d456fa48_1   query      SUCCESS   09 Aug 13:48:57   0:00:00.369000  
  bqjob_r186c01fe8ed94868_00000173d456456d_1   query      SUCCESS   09 Aug 13:48:11   0:00:00.498000  
  bqjob_r596685d70d42d244_00000173d4562970_1   query      SUCCESS   09 Aug 13:48:03   0:00:00.438000  
  bqjob_r33a3a095f66b8e5b_00000173d4555421_1   query      SUCCESS   09 Aug 13:47:09   0:00:00.546000  
  bqjob_r59e6252138b0f835_00000173d455045b_1   query      SUCCESS   09 Aug 13:46:48   0:00:00.503000  
  bqjob_r42ce7c4106c80940_00000173d454bbe7_1   query      SUCCESS   09 Aug 13:46:30   0:00:00.630000  
  job_cMB49WraYXFgV1hFl36-3OUo5eVk             query      SUCCESS   09 Aug 13:44:02   0:00:00.277000       &amp;lt;-- query1
  bqjob_r5cc0550b6f367c57_00000173d450769b_1   query      SUCCESS   09 Aug 13:41:51   0:00:02.041000     &amp;lt;-- query2
  bqjob_r4ac46e2feef9213a_00000173d41eec48_1   query      FAILURE   09 Aug 12:47:43   0:00:00        


bq show -j job_cMB49WraYXFgV1hFl36-3OUo5eVk
  Job Type    State      Start Time         Duration                              User Email                           Bytes Processed   Bytes Billed   Billing Tier   Labels  
 ---------- --------- ----------------- ---------------- ------------------------------------------------------------ ----------------- -------------- -------------- -------- 
  query      SUCCESS   09 Aug 13:44:02   0:00:00.277000   example-dev-svc@example-dev-284123.iam.gserviceaccount.com   0                 0                                     

bq show -j bqjob_r5cc0550b6f367c57_00000173d450769b_1
  Job Type    State      Start Time         Duration                User Email             Bytes Processed   Bytes Billed   Billing Tier   Labels  
 ---------- --------- ----------------- ---------------- -------------------------------- ----------------- -------------- -------------- -------- 
  query      SUCCESS   09 Aug 13:41:51   0:00:02.041000   kristofferson.a.arao@gmail.com   903989528         904921088      1                      



bq --format=prettyjson show -j job_cMB49WraYXFgV1hFl36-3OUo5eVk | grep query 
    &quot;query&quot;: {
      &quot;query&quot;: &quot;SELECT    start_station_name   , AVG(duration) as duration   , COUNT(duration) as num_trips FROM `bigquery-public-data`.london_bicycles.cycle_hire GROUP BY start_station_name  ORDER BY num_trips DESC  LIMIT 5&quot;,


bq --format=prettyjson show -j bqjob_r5cc0550b6f367c57_00000173d450769b_1 | grep query
    &quot;query&quot;: {
      &quot;query&quot;: &quot;SELECT start_station_name , AVG(duration) as duration , COUNT(duration) as num_trips FROM `bigquery-public-data`.london_bicycles.cycle_hire GROUP BY start_station_name ORDER BY num_trips DESC LIMIT 5&quot;,



bq --format=prettyjson show -j job_cMB49WraYXFgV1hFl36-3OUo5eVk &amp;gt; query1.sql
bq --format=prettyjson show -j bqjob_r5cc0550b6f367c57_00000173d450769b_1 &amp;gt; query2.sql
diff -y query1.sql query2.sql | less

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; backup and restore dataset and tables &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/tree/master/blogs/bigquery_backup&quot; href=&quot;https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/tree/master/blogs/bigquery_backup&quot; class=&quot;externalLink&quot;&gt;https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/tree/master/blogs/bigquery_backup&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;pre&gt;# backup one table 
# This saves a schema.json, a tabledef.json, and extracted data in AVRO format to GCS
./bq_backup.py --input dataset.tablename --output gs://BUCKET/backup


# backup all the tables in a data set
python bigquery_backup.py --input ch04 --output gs://example_bucket-dev/backup


# restore per table 
gsutil ls gs://example_bucket-dev/backup/ch04
gs://example_bucket-dev/backup/ch04/LOAN_STATUS_SED_OFFER/
gs://example_bucket-dev/backup/ch04/college_scorecard/
gs://example_bucket-dev/backup/ch04/college_scorecard3/
gs://example_bucket-dev/backup/ch04/college_scorecard3a/
gs://example_bucket-dev/backup/ch04/college_scorecard_etl/

python bigquery_restore.py --input gs://example_bucket-dev/backup/ch04/college_scorecard/ --output ch05
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; bq insert to table &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;bq insert ch05.rentals_last_hour data.json
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; bq extract from table &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/exporting-data#exporting_data_stored_in&quot; href=&quot;https://cloud.google.com/bigquery/docs/exporting-data#exporting_data_stored_in&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/exporting-data#exporting_data_stored_in&lt;/a&gt;&lt;br&gt;&lt;pre&gt;bq extract --format=json ch05.bad_bikes gs://bad_bikes.json
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; SCRIPTING - data types&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types&quot; href=&quot;https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;pre&gt;  BigQuery Data Types
  Data Type     Description
  ============= =======================================
  INT64         range: -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807
  NUMERIC       range: -99999999999999999999999999999.999999999 to 99999999999999999999999999999.999999999
  FLOAT64       Double precision (approximate) decimal values
  BOOL          Represented by the keywords TRUE and FALSE (case insensitive)
  STRING        Variable-length character (Unicode) values must be UTF-8 encoded  no way to set per column max length
  BYTES         Variable-length binary data
  DATE          range: 0001-01-01 to 9999-12-31  No time part
  DATETIME      range: 0001-01-01 00:00:00 to 9999-12-31 23:59:59.999999  Date and time parts to the microsecond
  TIME          range: 00:00:00 to 23:59:59.99999  No date part
  TIMESTAMP     range: 0001-01-01 00:00:00 to 9999-12-31 23:59:59.999999 UTC  Date and time parts to the microsecond and timezone

  Data Type     Size
  ============= =======================================
  INT64         8 bytes
  FLOAT64       8 bytes
  NUMERIC       16 bytes
  BBOOLEAN      1 byte
  STRING        2 bytes + the UTF-8 encoded string size
  BYTES         2 bytes + the number of bytes in the value
  DATE          8 bytes
  DATETIME      8 bytes
  TIME          8 bytes
  TIMESTAMP     8 bytes
  STRUCT/RECORD 0 bytes + the size of the contained fields
    Notes:
      https://cloud.google.com/bigquery/pricing#data
      Null values for any data type are use 0 bytes
=====================================================================
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; SCRIPTING - use bq CLI to execute a query with bind parameters&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;echo &quot;select @bv_1 as col1, @bv_2 as col2;&quot; &amp;gt; test.sql
bq query --use_legacy_sql=False --nodry_run --parameter=bv_1::USA --parameter=bv_2::PH  &amp;lt; test.sql

Waiting on bqjob_r5bea3ffac049a315_00000173d6fabb8b_1 ... (0s) Current status: DONE   
+------+------+
| col1 | col2 |
+------+------+
| USA  | PH   |
+------+------+
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; SCRIPTING - parameterized queries using the Python Cloud Client API&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;..&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;</description>
<category>gcp dev tools</category>
<category>gcp bigquery</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bgcp%20bq%20commandline%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 17:05:00 GMT</pubDate>

</item>
<item>
<title>sql converter - teradata to bigquery / snowflake</title>
<description>&lt;a target=&quot;_blank&quot; title=&quot;External link to http://www.sqlines.com/online&quot; href=&quot;http://www.sqlines.com/online&quot; class=&quot;externalLink&quot;&gt;http://www.sqlines.com/online&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://roboquery.com/&quot; href=&quot;https://roboquery.com/&quot; class=&quot;externalLink&quot;&gt;https://roboquery.com/&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://www.compilerworks.com/&quot; href=&quot;https://www.compilerworks.com/&quot; class=&quot;externalLink&quot;&gt;https://www.compilerworks.com/&lt;/a&gt;</description>
<category>bq performance</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bsql%20converter%20-%20teradata%20to%20bigquery%20%2F%20snowflake%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:45:00 GMT</pubDate>

</item>
<item>
<title>bq query labeles - DBMS_APPLICATION_INFO</title>
<description>&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/51133540/how-do-i-use-labels-in-big-query-queries-to-track-cost&quot; href=&quot;https://stackoverflow.com/questions/51133540/how-do-i-use-labels-in-big-query-queries-to-track-cost&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/51133540/how-do-i-use-labels-in-big-query-queries-to-track-cost&lt;/a&gt;&lt;br&gt;&lt;pre&gt;bq query \
--nouse_legacy_sql \
--label foo:bar \
'SELECT COUNT(*) FROM `bigquery-public-data`.samples.shakespeare'


Then, you can issue below filter in Cloud logging

resource.type=&quot;bigquery_resource&quot;
protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.labels.foo=&quot;bar&quot;

https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfiguration
&lt;/pre&gt;</description>
<category>bq performance</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20query%20labeles%20-%20DBMS_APPLICATION_INFO%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:24:00 GMT</pubDate>

</item>
<item>
<title>.bq database engine - dremel</title>
<description>Dremel: Interactive Analysis of &lt;a tiddlylink=&quot;Web-Scale&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#Web-Scale&quot; href=&quot;http://karlarao.tiddlyspot.com#Web-Scale&quot; class=&quot;externalLink null&quot;&gt;Web-Scale&lt;/a&gt; Datasets (with cost based optimizer)&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf&quot; href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf&quot; class=&quot;externalLink&quot;&gt;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf&lt;/a&gt;</description>
<category>bq performance</category>
<link>http://karlarao.tiddlyspot.com#%5B%5B.bq%20database%20engine%20-%20dremel%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:21:00 GMT</pubDate>

</item>
<item>
<title>bigquery client install</title>
<description>&lt;div class=&quot;dcTOC&quot;&gt;&lt;a class=&quot;toggleButton&quot; title=&quot;show/collapse table of contents&quot; href=&quot;javascript:;&quot;&gt;/* Table of Contents */&lt;/a&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' install packages', event)&quot;&gt; install packages&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', '  configure credentials ', event)&quot;&gt;  configure credentials &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; install packages&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;
 pip install google-cloud-bigquery
  pip install google-cloud-bigquery-datatransfer
  pip install google-cloud-bigquery-storage


pip list | grep google
google-api-core                    1.22.0
google-auth                        1.19.2
google-cloud-bigquery              1.26.0
google-cloud-bigquery-datatransfer 1.1.0
google-cloud-bigquery-storage      1.0.0
google-cloud-core                  1.3.0
google-resumable-media             0.5.1
googleapis-common-protos           1.52.0

&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt;  configure credentials &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;pre&gt;python test.py 
Traceback (most recent call last):
  File &quot;test.py&quot;, line 5, in &amp;lt;module&amp;gt;
    client = bigquery.Client()
  File &quot;/Users/kristofferson.a.arao/.pyenv/versions/py385/lib/python3.8/site-packages/google/cloud/bigquery/client.py&quot;, line 178, in __init__
    super(Client, self).__init__(
  File &quot;/Users/kristofferson.a.arao/.pyenv/versions/py385/lib/python3.8/site-packages/google/cloud/client.py&quot;, line 226, in __init__
    _ClientProjectMixin.__init__(self, project=project)
  File &quot;/Users/kristofferson.a.arao/.pyenv/versions/py385/lib/python3.8/site-packages/google/cloud/client.py&quot;, line 178, in __init__
    project = self._determine_default(project)
  File &quot;/Users/kristofferson.a.arao/.pyenv/versions/py385/lib/python3.8/site-packages/google/cloud/client.py&quot;, line 193, in _determine_default
    return _determine_default_project(project)
  File &quot;/Users/kristofferson.a.arao/.pyenv/versions/py385/lib/python3.8/site-packages/google/cloud/_helpers.py&quot;, line 186, in _determine_default_project
    _, project = google.auth.default()
  File &quot;/Users/kristofferson.a.arao/.pyenv/versions/py385/lib/python3.8/site-packages/google/auth/_default.py&quot;, line 338, in default
    raise exceptions.DefaultCredentialsError(_HELP_MESSAGE)
google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started
&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/reference/libraries&quot; href=&quot;https://cloud.google.com/bigquery/docs/reference/libraries&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/reference/libraries&lt;/a&gt;&lt;br&gt;</description>
<category>gcp authentication</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbigquery%20client%20install%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:19:00 GMT</pubDate>

</item>
<item>
<title>bq public datasets</title>
<description>&lt;pre&gt;
-- covid data 
SELECT
  date, SUM(confirmed) num_reports
FROM `bigquery-public-data.covid19_open_data.compatibility_view`
WHERE ST_Distance(ST_GeogPoint(longitude, latitude),  
                  ST_GeogPoint(103.8, 1.4)) &amp;lt; 200*1000     -- 200km
GROUP BY date
HAVING num_reports IS NOT NULL AND num_reports &amp;gt; 0
ORDER BY date ASC
;


-- nyc bike data 
-- one-way rentals by year, month
SELECT 
  EXTRACT(YEAR FROM starttime) AS year,
  EXTRACT(MONTH FROM starttime) AS month,
  COUNT(starttime) AS number_one_way
FROM
  `bigquery-public-data`.new_york_citibike.citibike_trips
WHERE
  start_station_name != end_station_name
GROUP BY year, month
ORDER BY year ASC, month ASC
;


&lt;/pre&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;.</description>
<category>bigquery SQL, bq sql</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20public%20datasets%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:18:00 GMT</pubDate>

</item>
<item>
<title>bq pricing billing limitations quotas</title>
<description>&lt;div class=&quot;dcTOC&quot;&gt;&lt;a class=&quot;toggleButton&quot; title=&quot;show/collapse table of contents&quot; href=&quot;javascript:;&quot;&gt;/* Table of Contents */&lt;/a&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' docs', event)&quot;&gt; docs&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' how ', event)&quot;&gt; how &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' limitations', event)&quot;&gt; limitations&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' quotas and limits', event)&quot;&gt; quotas and limits&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; docs&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/slots&quot; href=&quot;https://cloud.google.com/bigquery/docs/slots&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/slots&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; how &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;ul&gt;&lt;li&gt; it is to this project that storage costs for tables in this dataset will be billed (queries are charged to the project of the querier)&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; limitations&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;ul&gt;&lt;li&gt; Be careful when choosing a region for loading data: as of this writing, queries cannot join tables held in different regions&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; quotas and limits&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/quotas&quot; href=&quot;https://cloud.google.com/bigquery/quotas&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/quotas&lt;/a&gt;&lt;br&gt;&lt;br&gt;..</description>
<category>bq pricing</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20pricing%20billing%20limitations%20quotas%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:17:00 GMT</pubDate>

</item>
<item>
<title>bq security - security, grants, roles</title>
<description></description>
<category>gcp bigquery</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20security%20-%20security%2C%20grants%2C%20roles%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:17:00 GMT</pubDate>

</item>
<item>
<title>bq encryption</title>
<description>&lt;div class=&quot;dcTOC&quot;&gt;&lt;a class=&quot;toggleButton&quot; title=&quot;show/collapse table of contents&quot; href=&quot;javascript:;&quot;&gt;/* Table of Contents */&lt;/a&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' doc ', event)&quot;&gt; doc &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' AEAD encryption', event)&quot;&gt; AEAD encryption&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' crypto shredding ', event)&quot;&gt; crypto shredding &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' Customer-Managed Encryption Keys for BigQuery', event)&quot;&gt; Customer-Managed Encryption Keys for BigQuery&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' other references', event)&quot;&gt; other references&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; doc &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/encryption-at-rest&quot; href=&quot;https://cloud.google.com/bigquery/docs/encryption-at-rest&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/encryption-at-rest&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; AEAD encryption&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://github.com/google/tink&quot; href=&quot;https://github.com/google/tink&quot; class=&quot;externalLink&quot;&gt;https://github.com/google/tink&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/reference/standard-sql/aead-encryption-concepts&quot; href=&quot;https://cloud.google.com/bigquery/docs/reference/standard-sql/aead-encryption-concepts&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/reference/standard-sql/aead-encryption-concepts&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; crypto shredding &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://medium.com/google-cloud/bigquery-encryption-functions-part-i-data-deletion-retention-with-crypto-shredding-7085ecf6e53f&quot; href=&quot;https://medium.com/google-cloud/bigquery-encryption-functions-part-i-data-deletion-retention-with-crypto-shredding-7085ecf6e53f&quot; class=&quot;externalLink&quot;&gt;https://medium.com/google-cloud/bigquery-encryption-functions-part-i-data-deletion-retention-with-crypto-shredding-7085ecf6e53f&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://medium.com/google-cloud/end-to-end-crypto-shredding-part-ii-data-deletion-retention-with-crypto-shredding-a67f5300a8c8&quot; href=&quot;https://medium.com/google-cloud/end-to-end-crypto-shredding-part-ii-data-deletion-retention-with-crypto-shredding-a67f5300a8c8&quot; class=&quot;externalLink&quot;&gt;https://medium.com/google-cloud/end-to-end-crypto-shredding-part-ii-data-deletion-retention-with-crypto-shredding-a67f5300a8c8&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; &lt;a tiddlylink=&quot;Customer-Managed&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#Customer-Managed&quot; href=&quot;http://karlarao.tiddlyspot.com#Customer-Managed&quot; class=&quot;externalLink null&quot;&gt;Customer-Managed&lt;/a&gt; Encryption Keys for &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt;&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a tiddlylink=&quot;Customer-Managed&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#Customer-Managed&quot; href=&quot;http://karlarao.tiddlyspot.com#Customer-Managed&quot; class=&quot;externalLink null&quot;&gt;Customer-Managed&lt;/a&gt; Encryption Keys for &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; &lt;a target=&quot;_blank&quot; title=&quot;External link to https://www.youtube.com/watch?v=-dlv9wJheF8&quot; href=&quot;https://www.youtube.com/watch?v=-dlv9wJheF8&quot; class=&quot;externalLink&quot;&gt;https://www.youtube.com/watch?v=-dlv9wJheF8&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/customer-managed-encryption&quot; href=&quot;https://cloud.google.com/bigquery/docs/customer-managed-encryption&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/customer-managed-encryption&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; other references&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://github.com/google/encrypted-bigquery-client/blob/master/tutorial.md&quot; href=&quot;https://github.com/google/encrypted-bigquery-client/blob/master/tutorial.md&quot; class=&quot;externalLink&quot;&gt;https://github.com/google/encrypted-bigquery-client/blob/master/tutorial.md&lt;/a&gt;    &amp;lt;- EBQ (old stuff)&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/53274323/field-level-encryption-in-big-query?rq=1&quot; href=&quot;https://stackoverflow.com/questions/53274323/field-level-encryption-in-big-query?rq=1&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/53274323/field-level-encryption-in-big-query?rq=1&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/57723775/how-can-i-decrypt-columns-in-bigquery&quot; href=&quot;https://stackoverflow.com/questions/57723775/how-can-i-decrypt-columns-in-bigquery&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/57723775/how-can-i-decrypt-columns-in-bigquery&lt;/a&gt;&lt;br&gt;</description>
<category>bq security - security, grants, roles</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20encryption%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:15:00 GMT</pubDate>

</item>
<item>
<title>bq performance</title>
<description>&lt;a target=&quot;_blank&quot; title=&quot;External link to https://rittmananalytics.com/blog/tag/Bigquery&quot; href=&quot;https://rittmananalytics.com/blog/tag/Bigquery&quot; class=&quot;externalLink&quot;&gt;https://rittmananalytics.com/blog/tag/Bigquery&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt; Drill to Detail Ep.64 ‘Google &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt;, BI Engine and the Future of Data Warehousing’ with Special Guest Jordan Tigani &lt;a target=&quot;_blank&quot; title=&quot;External link to https://rittmananalytics.com/drilltodetail/2019/4/29/drill-to-detail-ep64-google-bigquery-bi-engine-and-the-future-of-data-warehousing-with-special-guest-jordan-tigani&quot; href=&quot;https://rittmananalytics.com/drilltodetail/2019/4/29/drill-to-detail-ep64-google-bigquery-bi-engine-and-the-future-of-data-warehousing-with-special-guest-jordan-tigani&quot; class=&quot;externalLink&quot;&gt;https://rittmananalytics.com/drilltodetail/2019/4/29/drill-to-detail-ep64-google-bigquery-bi-engine-and-the-future-of-data-warehousing-with-special-guest-jordan-tigani&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;</description>
<category>gcp bigquery</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20performance%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:14:00 GMT</pubDate>

</item>
<item>
<title>bq data partitioning and clustering</title>
<description>&lt;a target=&quot;_blank&quot; title=&quot;External link to https://rittmananalytics.com/blog/2018/08/27/date-partitioning-and-table-clustering-in-google-bigquery-and-looker-pdts&quot; href=&quot;https://rittmananalytics.com/blog/2018/08/27/date-partitioning-and-table-clustering-in-google-bigquery-and-looker-pdts&quot; class=&quot;externalLink&quot;&gt;https://rittmananalytics.com/blog/2018/08/27/date-partitioning-and-table-clustering-in-google-bigquery-and-looker-pdts&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://rittmananalytics.com/blog/tag/Big+Data&quot; href=&quot;https://rittmananalytics.com/blog/tag/Big+Data&quot; class=&quot;externalLink&quot;&gt;https://rittmananalytics.com/blog/tag/Big+Data&lt;/a&gt;</description>
<category>bq performance</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20data%20partitioning%20and%20clustering%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:10:00 GMT</pubDate>

</item>
<item>
<title>bigquery teradata migration</title>
<description>&lt;div class=&quot;dcTOC&quot;&gt;&lt;a class=&quot;toggleButton&quot; title=&quot;show/collapse table of contents&quot; href=&quot;javascript:;&quot;&gt;/* Table of Contents */&lt;/a&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' teradata official doc ', event)&quot;&gt; teradata official doc &lt;/a&gt;&lt;/span&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' SET and MULTISET', event)&quot;&gt; SET and MULTISET&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' sql converter ', event)&quot;&gt; sql converter &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' bigquery datatypes teradata', event)&quot;&gt; bigquery datatypes teradata&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' query band - for sql tracking ', event)&quot;&gt; query band - for sql tracking &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; teradata official doc &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;h2&gt; SET and MULTISET&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h2&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://docs.teradata.com/reader/rgAb27O_xRmMVc_aQq2VGw/ZAb34NX2bdG0HpkqUsmUmQ&quot; href=&quot;https://docs.teradata.com/reader/rgAb27O_xRmMVc_aQq2VGw/ZAb34NX2bdG0HpkqUsmUmQ&quot; class=&quot;externalLink&quot;&gt;https://docs.teradata.com/reader/rgAb27O_xRmMVc_aQq2VGw/ZAb34NX2bdG0HpkqUsmUmQ&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; sql converter &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://roboquery.com/app/&quot; href=&quot;https://roboquery.com/app/&quot; class=&quot;externalLink&quot;&gt;https://roboquery.com/app/&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://roboquery.com/&quot; href=&quot;https://roboquery.com/&quot; class=&quot;externalLink&quot;&gt;https://roboquery.com/&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; bigquery datatypes teradata&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/solutions/migration/dw2bq/td2bq/td-bq-sql-translation-reference-tables&quot; href=&quot;https://cloud.google.com/solutions/migration/dw2bq/td2bq/td-bq-sql-translation-reference-tables&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/solutions/migration/dw2bq/td2bq/td-bq-sql-translation-reference-tables&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types&quot; href=&quot;https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; query band - for sql tracking &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;Using the BLOCKCOMPRESSION Reserved Query Band &lt;a target=&quot;_blank&quot; title=&quot;External link to https://docs.teradata.com/reader/scPHvjfglIlB8F70YliLAw/~iM4rfkST9SWmEwqW4jXPQ&quot; href=&quot;https://docs.teradata.com/reader/scPHvjfglIlB8F70YliLAw/%7EiM4rfkST9SWmEwqW4jXPQ&quot; class=&quot;externalLink&quot;&gt;https://docs.teradata.com/reader/scPHvjfglIlB8F70YliLAw/~iM4rfkST9SWmEwqW4jXPQ&lt;/a&gt;</description>
<category>bq migration</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbigquery%20teradata%20migration%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 16:03:00 GMT</pubDate>

</item>
<item>
<title>bq data generator</title>
<description>&lt;a target=&quot;_blank&quot; title=&quot;External link to https://github.com/GoogleCloudPlatform/professional-services/tree/master/examples/dataflow-data-generator&quot; href=&quot;https://github.com/GoogleCloudPlatform/professional-services/tree/master/examples/dataflow-data-generator&quot; class=&quot;externalLink&quot;&gt;https://github.com/GoogleCloudPlatform/professional-services/tree/master/examples/dataflow-data-generator&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://medium.com/google-cloud/yet-another-way-to-generate-fake-datasets-in-bigquery-93ee87c1008f&quot; href=&quot;https://medium.com/google-cloud/yet-another-way-to-generate-fake-datasets-in-bigquery-93ee87c1008f&quot; class=&quot;externalLink&quot;&gt;https://medium.com/google-cloud/yet-another-way-to-generate-fake-datasets-in-bigquery-93ee87c1008f&lt;/a&gt;&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://datarunsdeep.com.au/blog/flying-beagle/how-consistently-select-randomly-distributed-sample-rows-bigquery-table&quot; href=&quot;https://datarunsdeep.com.au/blog/flying-beagle/how-consistently-select-randomly-distributed-sample-rows-bigquery-table&quot; class=&quot;externalLink&quot;&gt;https://datarunsdeep.com.au/blog/flying-beagle/how-consistently-select-randomly-distributed-sample-rows-bigquery-table&lt;/a&gt;</description>
<category>bq performance</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20data%20generator%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 15:59:00 GMT</pubDate>

</item>
<item>
<title>gcp DLP scan - Cloud Data Loss Prevention</title>
<description>&lt;div class=&quot;dcTOC&quot;&gt;&lt;a class=&quot;toggleButton&quot; title=&quot;show/collapse table of contents&quot; href=&quot;javascript:;&quot;&gt;/* Table of Contents */&lt;/a&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' doc', event)&quot;&gt; doc&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; doc&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/scan-with-dlp&quot; href=&quot;https://cloud.google.com/bigquery/docs/scan-with-dlp&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/scan-with-dlp&lt;/a&gt;&lt;br&gt;</description>
<category>bq security - security, grants, roles</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bgcp%20DLP%20scan%20-%20Cloud%20Data%20Loss%20Prevention%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 15:55:00 GMT</pubDate>

</item>
<item>
<title>bq column level security</title>
<description>&lt;div class=&quot;dcTOC&quot;&gt;&lt;a class=&quot;toggleButton&quot; title=&quot;show/collapse table of contents&quot; href=&quot;javascript:;&quot;&gt;/* Table of Contents */&lt;/a&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' doc ', event)&quot;&gt; doc &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt; &lt;br&gt;&lt;br&gt;&lt;h1&gt; doc &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/column-level-security-intro&quot; href=&quot;https://cloud.google.com/bigquery/docs/column-level-security-intro&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/column-level-security-intro&lt;/a&gt;</description>
<category>bq security - security, grants, roles</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20column%20level%20security%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 15:53:00 GMT</pubDate>

</item>
<item>
<title>bq access controls</title>
<description>&lt;div class=&quot;dcTOC&quot;&gt;&lt;a class=&quot;toggleButton&quot; title=&quot;show/collapse table of contents&quot; href=&quot;javascript:;&quot;&gt;/* Table of Contents */&lt;/a&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' doc ', event)&quot;&gt; doc &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' cross dataset access', event)&quot;&gt; cross dataset access&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;javascript:;&quot; onclick=&quot;window.scrollToHeading('', ' limit access to datasets', event)&quot;&gt; limit access to datasets&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; doc &lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/access-control-examples&quot; href=&quot;https://cloud.google.com/bigquery/docs/access-control-examples&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/access-control-examples&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; cross dataset access&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/dataset-access-controls#controlling_access_to_a_dataset&quot; href=&quot;https://cloud.google.com/bigquery/docs/dataset-access-controls#controlling_access_to_a_dataset&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/dataset-access-controls#controlling_access_to_a_dataset&lt;/a&gt;&lt;br&gt;&lt;br&gt;    on the dataset , click SHARE and add the service account ID &quot;client_email&quot;: &quot;example-dev-svc@iam.gserviceaccount.com&quot;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;h1&gt; limit access to datasets&lt;div style=&quot;font-size: 0.5em; color: blue;&quot;&gt;&lt;a class=&quot;dcTOCTop&quot; title=&quot;Go to top of tiddler&quot; href=&quot;javascript:;&quot;&gt; [top]&lt;/a&gt;&lt;/div&gt;&lt;/h1&gt;&lt;br&gt;Is it possible to limit a Google service account to specific &lt;a tiddlylink=&quot;BigQuery&quot; refresh=&quot;link&quot; target=&quot;_blank&quot; title=&quot;External link to http://karlarao.tiddlyspot.com#BigQuery&quot; href=&quot;http://karlarao.tiddlyspot.com#BigQuery&quot; class=&quot;externalLink null&quot;&gt;BigQuery&lt;/a&gt; datasets within a project&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://stackoverflow.com/questions/59736056/is-it-possible-to-limit-a-google-service-account-to-specific-bigquery-datasets-w&quot; href=&quot;https://stackoverflow.com/questions/59736056/is-it-possible-to-limit-a-google-service-account-to-specific-bigquery-datasets-w&quot; class=&quot;externalLink&quot;&gt;https://stackoverflow.com/questions/59736056/is-it-possible-to-limit-a-google-service-account-to-specific-bigquery-datasets-w&lt;/a&gt;&lt;br&gt;</description>
<category>bq security - security, grants, roles</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20access%20controls%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 15:52:00 GMT</pubDate>

</item>
<item>
<title>bq authorized views - limit access to users</title>
<description>authorized views - limit access to users&lt;br&gt;&lt;a target=&quot;_blank&quot; title=&quot;External link to https://cloud.google.com/bigquery/docs/share-access-views&quot; href=&quot;https://cloud.google.com/bigquery/docs/share-access-views&quot; class=&quot;externalLink&quot;&gt;https://cloud.google.com/bigquery/docs/share-access-views&lt;/a&gt;</description>
<category>bq security - security, grants, roles</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20authorized%20views%20-%20limit%20access%20to%20users%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 14:30:00 GMT</pubDate>

</item>
<item>
<title>bq federated query</title>
<description></description>
<category>gcp bigquery</category>
<link>http://karlarao.tiddlyspot.com#%5B%5Bbq%20federated%20query%5D%5D</link>
<pubDate>Fri, 14 Aug 2020 14:17:00 GMT</pubDate>

</item>
</channel>
</rss>